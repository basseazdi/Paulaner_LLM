"""config_campaignreporting.json_generator.prompts
=================================================
This module stores all reusable prompt templates and few shot examples
used by the *campaignâ€‘reporting* pipeline.  They are imported by
`json_generator/main.py` (see the caller code in the userâ€™s snippet).

Every string below is deliberately kept in **plain UTFâ€‘8** with English
directives, because the LLM (Claude 3.5 Sonnet via Bedrock) performs best
when instructions are unambiguous and the evaluation criteria are crystal
clear.

#Â Variables exported
--------------------
- **rules**:      Master instruction set â€“ ALWAYS prepend to a chat.
- **first_example**, **second_example**, **third_example**: user messages
  containing a groundâ€‘truth JSON (`{json_X}`) and the JSON obtained from
  an Excel briefing (`{brf_X}`) so the model can learn the comparison
  task.  Each of these is paired with **expected_response_#**, which
  shows the ideal assistant answer.
- **new_request**: template used in production â€“ placeholders
  `{clean_briefing}` and `{json}` will be filled at runtime.

The comparison logic focuses on the **most businessâ€‘critical sections**:
  * Start / end dates of the commercial activity.
  * Main description.
  * Successâ€‘criteria list (including each metricâ€™s *name*, *function*,
    *min_amount*, and *time window*).
  * Customer segmentation: must be either *null* or an explicit, valid
    segment object.

All example responses follow the same output convention so that the
calling code can parse them deterministically.
"""

###########################################################################
# 1. GLOBAL RULES ##########################################################
###########################################################################

rules: str = """
You are a *QA auditor* in charge of validating that two JSON documents   
representing the same marketing campaign are **semantically identical**.   
The ğŸŸ¢ *reference JSON* comes from Adobe Campaign and is assumed to be
correct; the ğŸŸ  *briefing JSON* was generated from an Excel sheet.  Your
job is to return a **bulletâ€‘list of discrepancies** or the literal token
`NO_DIFF` when both documents match.

ğŸ” **Check the following elements (and nothing else):**
1. **commercial_activity.start_date** and **commercial_activity.end_date**   
   â€“ must exist and be ISO formatted (`YYYY-MM-DD`).
2. **description** â€“ plainâ€‘text description of the action.  Ignore case and
   extra whitespace when comparing.
3. **success_criteria / metrics** â€“ treat as an *unordered* list. Each
   entry must match on *metric_name*, *function*, *min_amount* (numeric),
   and *time_window*.
4. **customer_segmentation** â€“ must be either `null` or a wellâ€‘formed
   object detailing the segment logic.  The two JSONs must agree.

âš™ï¸ **Output format:**
- If there is **no difference**, respond with the single token `NO_DIFF`.
- Otherwise, one bullet per issue.  Use JSONâ€‘path notation to locate the
  field, then a short description of the problem, e.g.:
  - ğŸ”´ `commercial_activity.start_date`: expected `2025-08-01`, got `2025-07-30`
  - ğŸŸ  `success_criteria[1].min_amount`: expected `500`, got `200`
- Use the icons **ğŸ”´** for blocking errors (dates, missing keys), **ğŸŸ ** for
  warnings (description mismatch, additional keys).
- Keep the entire answer **under 1200 tokens**.
"""

###########################################################################
# 2. FEWâ€‘SHOT EXAMPLES #####################################################
###########################################################################

# NOTE: the placeholders {json_1}, {brf_1}, etc. will be replaced at
# runtime with real documents so that the example remains reusable.

first_example: str = """
Below is **ExampleÂ 1**. Compare the reference JSON to the briefing JSON
and answer following the *RULES*.

----------------  ğŸŸ¢  REFERENCE JSON  ----------------
{json_1}
----------------  ğŸŸ   BRIEFING JSON   ----------------
{brf_1}
"""

# In this example the two files are identical â†’ expected response is
# simply NO_DIFF.
expected_response_1: str = """NO_DIFF"""

second_example: str = """
Below is **ExampleÂ 2**. Compare the reference JSON to the briefing JSON
and answer following the *RULES*.

----------------  ğŸŸ¢  REFERENCE JSON  ----------------
{json_2}
----------------  ğŸŸ   BRIEFING JSON   ----------------
{brf_2}
"""

# Here we illustrate a *date* mismatch and a *min_amount* issue.
expected_response_2: str = """
- ğŸ”´ `commercial_activity.start_date`: expected `2025-04-01`, got `2025-04-03`
- ğŸŸ  `success_criteria[0].min_amount`: expected `300`, got `250`
"""

third_example: str = """
Below is **ExampleÂ 3**. Compare the reference JSON to the briefing JSON
and answer following the *RULES*.

----------------  ğŸŸ¢  REFERENCE JSON  ----------------
{json_3}
----------------  ğŸŸ   BRIEFING JSON   ----------------
{brf_3}
"""

# This example focuses on *segmentation* and *description* differences.
expected_response_3: str = """
- ğŸŸ  `description`: texts differ after normalisation
- ğŸ”´ `customer_segmentation`: expected `null`, got an object defining a
      segment named *Clientesâ€‘Alta* (should be null for an â€œAbiertoâ€
      campaign)
"""

###########################################################################
# 3. RUNTIME REQUEST TEMPLATE #############################################
###########################################################################

# The pipeline fills the placeholders {clean_briefing} and {json} each
# time it needs a fresh comparison.

new_request: str = """
Please compare the two JSON documents below.  Use the **RULES** and return
`NO_DIFF` or a bullet list of discrepancies.

================  ğŸŸ   BRIEFING JSON   =================
{clean_briefing}
================  ğŸŸ¢  REFERENCE JSON  =================
{json}
"""
